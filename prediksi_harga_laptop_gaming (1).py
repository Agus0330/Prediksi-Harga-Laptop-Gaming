# -*- coding: utf-8 -*-
"""Prediksi Harga Laptop Gaming.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17-apk74T6ied4P7htWgf97SsDzwyd3aB
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score, confusion_matrix, classification_report
from sklearn.neural_network import MLPRegressor
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import GridSearchCV
from keras.callbacks import EarlyStopping, ModelCheckpoint

#Load the dataset
laptop = 'kliknklik_gaming_laptop.csv'
data = pd.read_csv(laptop)

# Display the first few rows of the dataset to ensure it's loaded correctly
print(data.head())

data.head(5)

data.shape

data.isna().sum()

# Drop columns with mostly missing values or not useful for the model
data_cleaned = data.drop(columns=['Berat', 'Dimensi', 'Keyboard', 'Microsoft Office', 'Warna'])

# Fill missing values with mode
data_cleaned = data_cleaned.fillna(data_cleaned.mode().iloc[0])

# Convert 'Harga' from string to numeric by removing 'Rp' and all dots
data_cleaned['Harga'] = data_cleaned['Harga'].replace({'Rp': '', '\.': '', ',': ''}, regex=True).astype(int)

print(data_cleaned.head())

# Ensure all values are strings
data_cleaned['Ukuran Layar'] = data_cleaned['Ukuran Layar'].astype(str)
data_cleaned['RAM'] = data_cleaned['RAM'].astype(str)

data_cleaned['Ukuran Layar'] = data_cleaned['Ukuran Layar'].str.replace(' Inch', '').str.replace('Inch', '').astype(int)
data_cleaned['RAM'] = data_cleaned['RAM'].str.replace(' GB', '').str.replace('GB', '').astype(int)

print(data_cleaned.head())

# Convert storage from strings to numerical values
def convert_storage(storage):
    if 'TB' in storage:
        return int(storage.replace('TB', '')) * 1000
    elif 'GB' in storage:
        return int(storage.replace('GB', ''))
    else:
        return 0

data_cleaned['Penyimpanan'] = data_cleaned['Penyimpanan'].apply(convert_storage)

print(data_cleaned.head())

# Check data types to ensure conversion was successful
print(data_cleaned.dtypes)

#cek unique value setiap kolom
for column in data_cleaned.columns:
    # Mencetak nama kolom
    print(f"Column: {column}")

    # Mencetak nilai unik dari kolom tersebut
    unique_values = data_cleaned[column].unique()
    print(unique_values)

    # Jarak antar kolom
    print("-"*30)

# Encode categorical columns
categorical_columns = ['Nama', 'Garansi', 'VGA', 'Tipe Penyimpanan', 'Processor', 'Sistem Operasi', 'Brand', 'Tipe Layar']
label_encoders = {}

for column in categorical_columns:
    le = LabelEncoder()
    data_cleaned[column] = le.fit_transform(data_cleaned[column])
    label_encoders[column] = le

print(data_cleaned.head())

# Assuming data_cleaned is your cleaned dataframe from the previous steps
correlation_matrix = data_cleaned.corr()

# Display the correlation matrix
print(correlation_matrix)

# Plot the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# Get the correlation of each feature with 'Harga'
correlation_with_harga = correlation_matrix['Harga'].abs().sort_values(ascending=False)

# Display the correlation values
print(correlation_with_harga)

# Select features with a correlation higher than a threshold (e.g., 0.5)
selected_features = correlation_with_harga[correlation_with_harga > 0.15].index.tolist()

# Ensure 'Harga' is included
if 'Harga' not in selected_features:
    selected_features.append('Harga')

# Display selected features
print("Selected features:", selected_features)

# Normalize the data
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data_cleaned)

# Split the data
X = data_scaled[:, 1:]  # features
y = data_scaled[:, 0]    # target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Reshape the data for LSTM
X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))
model.add(LSTM(50))
model.add(Dropout(0.2))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, validation_data=(X_test_lstm, y_test), verbose=1)

# Evaluate the LSTM model
y_pred_lstm = model.predict(X_test_lstm)
rmse_lstm = np.sqrt(mean_squared_error(y_test, y_pred_lstm))
print(f'LSTM RMSE: {rmse_lstm}')

# Calculate MSE
mse_lstm = mean_squared_error(y_test, y_pred_lstm)
print(f'LSTM MSE: {mse_lstm}')

# Calculate R-squared
r2_lstm = r2_score(y_test, y_pred_lstm)
print(f'LSTM R-squared: {r2_lstm}')

# Convert regression output to binary classification (example thresholding)
threshold = 0.5
y_pred_class = (y_pred_lstm.flatten() > threshold).astype(int)
y_test_class = (y_test > threshold).astype(int)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_class, y_pred_class)
class_report = classification_report(y_test_class, y_pred_class)

print("Confusion Matrix:")
print(conf_matrix)

print("\nClassification Report:")
print(class_report)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Build and evaluate the Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
print(f'Linear Regression RMSE: {rmse_lr}')

# Calculate additional metrics for Linear Regression
mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

print(f'Linear Regression MAE: {mse_lr}')
print(f'Linear Regression R-squared: {r2_lr}')

# Multi-layer Perceptron Model
mlp_model = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=200, random_state=42)
mlp_model.fit(X_train, y_train)
y_pred_mlp = mlp_model.predict(X_test)
rmse_mlp = np.sqrt(mean_squared_error(y_test, y_pred_mlp))
print(f'Multi-layer Perceptron RMSE: {rmse_mlp}')

# Calculate additional metrics for Multi-layer Perceptron
mse_mlp = mean_squared_error(y_test, y_pred_mlp)
r2_mlp = r2_score(y_test, y_pred_mlp)

print(f'Multi-layer Perceptron MSE: {mse_mlp}')
print(f'Multi-layer Perceptron R-squared: {r2_mlp}')

# Visualization for each model's predictions
plt.figure(figsize=(18, 6))

# LSTM
plt.subplot(1, 3, 1)
plt.scatter(y_test, y_pred_lstm, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('LSTM')

# Linear Regression
plt.subplot(1, 3, 2)
plt.scatter(y_test, y_pred_lr, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Linear Regression')

# Multi-layer Perceptron
plt.subplot(1, 3, 3)
plt.scatter(y_test, y_pred_mlp, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Multi-layer Perceptron')

plt.tight_layout()
plt.show()

# Comparative visualization
plt.figure(figsize=(10, 6))
plt.plot(y_test, label='Actual')
plt.plot(y_pred_lr, label='Linear Regression', alpha=0.7)
plt.plot(y_pred_mlp, label='MLP', alpha=0.7)
plt.plot(y_pred_lstm, label='LSTM', alpha=0.7)
plt.legend()
plt.title('Model Comparison')
plt.xlabel('Samples')
plt.ylabel('Harga')
plt.show()

# Residual plots
plt.figure(figsize=(18, 6))

# LSTM Residuals
plt.subplot(1, 3, 1)
sns.residplot(x=y_test, y=y_test - y_pred_lstm.flatten(), lowess=True, color="r")
plt.xlabel('Actual')
plt.ylabel('Residuals')
plt.title('LSTM Residuals')

# Linear Regression Residuals
plt.subplot(1, 3, 2)
sns.residplot(x=y_test, y=y_test - y_pred_lr, lowess=True, color="g")
plt.xlabel('Actual')
plt.ylabel('Residuals')
plt.title('Linear Regression Residuals')

# Multi-layer Perceptron Residuals
plt.subplot(1, 3, 3)
sns.residplot(x=y_test, y=y_test - y_pred_mlp, lowess=True, color="b")
plt.xlabel('Actual')
plt.ylabel('Residuals')
plt.title('MLP Residuals')

plt.tight_layout()
plt.show()

# Output the calculated metrics
print(f'LSTM RMSE: {rmse_lstm}, MSE: {mse_lstm}, R-squared: {r2_lstm}')
print(f'Linear Regression RMSE: {rmse_lr}, MSE: {mse_lr}, R-squared: {r2_lr}')
print(f'Multi-layer Perceptron RMSE: {rmse_mlp}, MSE: {mse_mlp}, R-squared: {r2_mlp}')

